# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Descri√ß√£o do Projeto

**RagBot 2.0** √© um chatbot RAG (Retrieval-Augmented Generation) desenvolvido para an√°lise inteligente de documentos legais, especificamente ac√≥rd√£os da SEFAZ Acre. O sistema permite que usu√°rios fa√ßam upload de m√∫ltiplos PDFs e conversem com o conte√∫do atrav√©s de perguntas em linguagem natural, recebendo respostas contextualizadas com cita√ß√µes das fontes.

### Objetivo
Facilitar a consulta e an√°lise de grandes volumes de documentos legais atrav√©s de interface conversacional, permitindo recupera√ß√£o r√°pida de informa√ß√µes espec√≠ficas sem necessidade de leitura manual completa dos documentos.

## Stack Tecnol√≥gica

### Backend (FastAPI)
- **FastAPI 0.116.1** - Framework web ass√≠ncrono para API REST
- **Uvicorn 0.35.0** - Servidor ASGI para FastAPI
- **Python 3.x** - Linguagem base

### RAG & LLM
- **LangChain 0.3.27** - Framework para orquestra√ß√£o RAG
  - `langchain-core 0.3.72` - Abstra√ß√µes e tipos base
  - `langchain-community 0.3.27` - Integra√ß√µes e loaders
  - `langchain-groq 0.3.6` - Cliente Groq API
  - `langchain-chroma 0.2.5` - Integra√ß√£o ChromaDB
  - `langchain-huggingface 0.3.1` - Modelos HuggingFace
  - `langchain-text-splitters 0.3.9` - Divis√£o de documentos
- **ChromaDB 1.0.15** - Banco de dados vetorial para embeddings
- **Groq 0.30.0** - Cliente para API Groq (LLM)

### Processamento de Documentos
- **PyPDF 5.9.0** - Extra√ß√£o de texto de PDFs
- **sentence-transformers 5.0.0** - Modelos de embeddings
- **transformers 4.54.1** - Biblioteca HuggingFace

### Frontend (Streamlit)
- **Streamlit 1.47.1** - Framework para interface web interativa
- **Requests 2.32.4** - Cliente HTTP para comunica√ß√£o com API

### Utilidades
- **python-dotenv 1.1.1** - Gerenciamento de vari√°veis de ambiente
- **loguru 0.7.3** / **logging** - Sistema de logs

### Modelos
- **LLM**: LLaMA3-70B-8192 (via Groq API)
- **Embeddings**: all-MiniLM-L12-v2 (HuggingFace, 384 dimens√µes)

## Arquitetura RAG

### Vis√£o Geral do Fluxo

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Streamlit  ‚îÇ  HTTP   ‚îÇ   FastAPI    ‚îÇ  Query  ‚îÇ   ChromaDB   ‚îÇ
‚îÇ   Frontend   ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ   Backend    ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ  Vectorstore ‚îÇ
‚îÇ  (Port 8501) ‚îÇ         ‚îÇ  (Port 8000) ‚îÇ         ‚îÇ              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚îÇ API Call
                                ‚ñº
                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                         ‚îÇ   Groq API   ‚îÇ
                         ‚îÇ  (LLaMA3)    ‚îÇ
                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Fase 1: Ingest√£o e Indexa√ß√£o

**Fluxo**: PDF Upload ‚Üí Extra√ß√£o ‚Üí Chunking ‚Üí Embedding ‚Üí Armazenamento

1. **Upload de PDFs** (`server/main.py:65` - endpoint `/upload_pdfs/`)
   - Usu√°rio faz upload de m√∫ltiplos PDFs via Streamlit
   - Frontend envia arquivos para API via `POST /upload_pdfs/`

2. **Processamento de PDFs** (`server/modules/pdf_handlers.py:13`)
   - Fun√ß√£o: `process_uploaded_pdf(file: UploadFile)`
   - PDFs salvos em `./uploaded_pdfs/`
   - `PyPDFLoader` extrai texto de todas as p√°ginas
   - Cada p√°gina ‚Üí `Document` LangChain com metadata (source, page)

3. **Chunking de Documentos** (`server/modules/load_vectorstore.py:33`)
   - `RecursiveCharacterTextSplitter`:
     - **chunk_size**: 1000 caracteres
     - **chunk_overlap**: 100 caracteres
   - Overlap mant√©m contexto entre chunks adjacentes
   - Evita cortar frases/conceitos no meio

4. **Gera√ß√£o de Embeddings** (`server/modules/load_vectorstore.py:38`)
   - Modelo: `all-MiniLM-L12-v2` (HuggingFace)
   - Cada chunk de texto ‚Üí vetor de 384 dimens√µes
   - Embeddings capturam significado sem√¢ntico
   - Execu√ß√£o em CPU (configur√°vel via `model_kwargs`)

5. **Armazenamento Vetorial** (`server/modules/load_vectorstore.py:44-57`)
   - ChromaDB persiste embeddings em `./chroma_store/`
   - L√≥gica incremental:
     - Se vectorstore existe: adiciona novos documentos
     - Se n√£o existe: cria novo banco de dados
   - Mant√©m metadata para rastreabilidade das fontes

6. **Atualiza√ß√£o da Chain** (`server/main.py:87`)
   - **CR√çTICO**: Chain RAG √© recriada ap√≥s cada upload
   - Garante que novas queries vejam documentos adicionados
   - Chain armazenada em vari√°vel global `chain`

### Fase 2: Recupera√ß√£o e Gera√ß√£o (Query)

**Fluxo**: Pergunta ‚Üí Embedding ‚Üí Busca Sem√¢ntica ‚Üí Contexto ‚Üí LLM ‚Üí Resposta

1. **Pergunta do Usu√°rio** (`client/components/chatUI.py:25`)
   - Usu√°rio digita pergunta na interface de chat
   - Frontend envia via `POST /ask/` com Form data

2. **Valida√ß√£o** (`server/main.py:99`)
   - Verifica se `chain` est√° inicializada
   - Retorna HTTP 400 se vectorstore n√£o est√° pronto

3. **Embedding da Query** (`server/modules/llm.py:24`)
   - Pergunta convertida em vetor usando MESMO modelo de embeddings
   - Consist√™ncia crucial para busca sem√¢ntica funcionar

4. **Busca Sem√¢ntica** (`server/modules/llm.py:24`)
   - Retriever configurado: `search_kwargs={'k': 3}`
   - ChromaDB busca 3 chunks mais similares
   - Usa similaridade coseno entre vetores
   - Retorna chunks ordenados por relev√¢ncia

5. **Constru√ß√£o do Prompt** (`server/modules/llm.py:30`)
   - `RetrievalQA` com `chain_type='stuff'`
   - "Stuff" = insere todos os chunks recuperados no prompt
   - Prompt impl√≠cito: "Baseado no contexto: [chunks], responda: [pergunta]"

6. **Gera√ß√£o da Resposta** (`server/modules/llm.py:15`)
   - LLM: `ChatGroq` com `llama3-70b-8192`
   - **Temperature**: 0.2 (respostas mais determin√≠sticas e focadas)
   - LLM recebe contexto + pergunta
   - Gera resposta fundamentada nos documentos

7. **Formata√ß√£o** (`server/modules/query_handlers.py:26`)
   - Extrai resposta textual
   - Coleta fontes dos `source_documents`
   - Retorna JSON: `{"response": "...", "sources": [...]}`

8. **Exibi√ß√£o** (`client/components/chatUI.py:41`)
   - Resposta renderizada no chat
   - Fontes citadas abaixo para auditabilidade
   - Hist√≥rico salvo em `st.session_state.messages`

### Componentes Chave do RAG

**Vectorstore (ChromaDB)**
- Banco de dados vetorial persistente
- Armazena embeddings + metadata + texto original
- Permite buscas por similaridade eficientes

**Retriever**
- Interface entre vectorstore e chain
- Configur√°vel: n√∫mero de documentos (k), tipo de busca
- Retorna documentos ranqueados por relev√¢ncia

**RetrievalQA Chain**
- Une retriever + LLM em pipeline √∫nico
- Orquestra: busca ‚Üí formata√ß√£o ‚Üí gera√ß√£o
- Retorna respostas + documentos fonte

**Global Chain State**
- `chain` = vari√°vel global em `server/main.py`
- Inicializada no startup se vectorstore existe
- Recriada ap√≥s uploads para incluir novos documentos

## Estrutura de Pastas

```
RagBot/
‚îÇ
‚îú‚îÄ‚îÄ client/                          # Frontend Streamlit
‚îÇ   ‚îú‚îÄ‚îÄ app.py                      # Ponto de entrada principal
‚îÇ   ‚îÇ                               # - Configura p√°gina Streamlit
‚îÇ   ‚îÇ                               # - Renderiza layout (sidebar + main)
‚îÇ   ‚îÇ                               # - Orquestra componentes
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ config.py                   # Configura√ß√µes do cliente
‚îÇ   ‚îÇ                               # - API_URL: endpoint do backend
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ components/                 # Componentes UI modulares
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chatUI.py              # Interface de chat principal
‚îÇ   ‚îÇ   ‚îÇ                          # - Gerencia hist√≥rico (session_state)
‚îÇ   ‚îÇ   ‚îÇ                          # - Renderiza mensagens user/assistant
‚îÇ   ‚îÇ   ‚îÇ                          # - Exibe fontes citadas
‚îÇ   ‚îÇ   ‚îÇ                          # - Chama ask_question() da API
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ upload.py              # Componente de upload de PDFs
‚îÇ   ‚îÇ   ‚îÇ                          # - File uploader (m√∫ltiplos PDFs)
‚îÇ   ‚îÇ   ‚îÇ                          # - Bot√£o "Upload to DB"
‚îÇ   ‚îÇ   ‚îÇ                          # - Chama upload_pdfs_api()
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ history_download.py    # Download do hist√≥rico
‚îÇ   ‚îÇ                               # - Formata hist√≥rico do chat
‚îÇ   ‚îÇ                               # - Bot√£o para download como .txt
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îî‚îÄ‚îÄ api.py                  # Cliente da API backend
‚îÇ                                   # - upload_pdfs_api(): envia PDFs
‚îÇ                                   # - ask_question(): envia perguntas
‚îÇ
‚îú‚îÄ‚îÄ server/                          # Backend FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ main.py                     # Aplica√ß√£o FastAPI principal
‚îÇ   ‚îÇ                               # - Define endpoints: /upload_pdfs/, /ask/, /test
‚îÇ   ‚îÇ                               # - Lifespan manager: inicializa chain no startup
‚îÇ   ‚îÇ                               # - Vari√°vel global: chain (estado RAG)
‚îÇ   ‚îÇ                               # - Middleware CORS para acesso cross-origin
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ logger.py                   # Sistema de logging
‚îÇ   ‚îÇ                               # - setup_logger(): configura logger
‚îÇ   ‚îÇ                               # - Formato: [timestamp] [level] - message
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ modules/                    # M√≥dulos funcionais do RAG
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf_handlers.py        # Processamento de PDFs
‚îÇ   ‚îÇ   ‚îÇ                          # - process_uploaded_pdf(): salva e extrai p√°ginas
‚îÇ   ‚îÇ   ‚îÇ                          # - Usa PyPDFLoader
‚îÇ   ‚îÇ   ‚îÇ                          # - Salva em ./uploaded_pdfs/
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ load_vectorstore.py    # Gerenciamento do vectorstore
‚îÇ   ‚îÇ   ‚îÇ                          # - add_documents_to_vectorstore(): chunking + embedding + persist
‚îÇ   ‚îÇ   ‚îÇ                          # - RecursiveCharacterTextSplitter
‚îÇ   ‚îÇ   ‚îÇ                          # - ChromaDB em ./chroma_store/
‚îÇ   ‚îÇ   ‚îÇ                          # - PERSIST_DIR: constante do caminho
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm.py                 # Configura√ß√£o da LLM chain
‚îÇ   ‚îÇ   ‚îÇ                          # - get_llm_chain(): cria RetrievalQA
‚îÇ   ‚îÇ   ‚îÇ                          # - ChatGroq com LLaMA3-70B
‚îÇ   ‚îÇ   ‚îÇ                          # - Retriever com k=3
‚îÇ   ‚îÇ   ‚îÇ                          # - Carrega GROQ_API_KEY do .env
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ query_handlers.py      # Execu√ß√£o de queries
‚îÇ   ‚îÇ                               # - query_chain(): invoca chain e formata resposta
‚îÇ   ‚îÇ                               # - Extrai response + sources
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt            # Depend√™ncias Python
‚îÇ
‚îú‚îÄ‚îÄ uploaded_pdfs/                   # PDFs enviados (criado em runtime)
‚îÇ                                   # - Armazena arquivos originais
‚îÇ                                   # - Usado como refer√™ncia de fonte
‚îÇ
‚îú‚îÄ‚îÄ chroma_store/                    # Banco vetorial ChromaDB (criado em runtime)
‚îÇ                                   # - Embeddings persistidos
‚îÇ                                   # - Metadata dos documentos
‚îÇ                                   # - √çndices de busca
‚îÇ
‚îú‚îÄ‚îÄ .env                            # Vari√°veis de ambiente (N√ÉO comitar!)
‚îÇ                                   # - GROQ_API_KEY
‚îÇ
‚îú‚îÄ‚îÄ .gitignore                      # Arquivos ignorados pelo git
‚îÇ                                   # - venv/, .env, uploaded_pdfs/, chroma_store/
‚îÇ
‚îî‚îÄ‚îÄ README.md                       # Instru√ß√µes b√°sicas de setup
```

### Arquivos Cr√≠ticos

**server/main.py** - Cora√ß√£o do backend
- Inicializa√ß√£o da chain no startup (lifespan manager)
- Endpoints REST
- Vari√°vel global `chain` mant√©m estado

**server/modules/load_vectorstore.py** - Gest√£o do conhecimento
- L√≥gica de chunking
- Cria√ß√£o/atualiza√ß√£o do vectorstore
- Configura√ß√£o de embeddings

**server/modules/llm.py** - Configura√ß√£o RAG
- Define modelo LLM
- Configura retriever (k=3)
- Monta RetrievalQA chain

**client/components/chatUI.py** - Interface principal
- Gerencia hist√≥rico com session_state
- Loop de conversa√ß√£o
- Exibi√ß√£o de respostas + fontes

## Configura√ß√£o

### Vari√°veis de Ambiente

Criar arquivo `.env` na raiz do projeto:

```bash
# Obrigat√≥rio: Chave da API Groq para acesso ao LLaMA3
GROQ_API_KEY=gsk_your_groq_api_key_here
```

**Como obter GROQ_API_KEY:**
1. Criar conta em https://console.groq.com
2. Acessar API Keys
3. Gerar nova chave
4. Copiar e colar no .env

### Configura√ß√µes do Cliente

**client/config.py:**
```python
API_URL = "http://127.0.0.1:8000"  # URL do backend
```

**Modificar quando deploying:**
- Desenvolvimento: `http://127.0.0.1:8000`
- Produ√ß√£o: `https://seu-dominio.com`

### Configura√ß√µes do RAG

**Par√¢metros ajust√°veis:**

| Par√¢metro | Arquivo | Linha | Valor Padr√£o | Descri√ß√£o |
|-----------|---------|-------|--------------|-----------|
| `chunk_size` | `load_vectorstore.py` | 33 | 1000 | Tamanho dos chunks (caracteres) |
| `chunk_overlap` | `load_vectorstore.py` | 33 | 100 | Sobreposi√ß√£o entre chunks |
| `k` (retrieval) | `llm.py` | 24 | 3 | N√∫mero de chunks recuperados |
| `temperature` | `llm.py` | 18 | 0.2 | Criatividade (0=determin√≠stico, 1=criativo) |
| `model_name` | `llm.py` | 17 | llama3-70b-8192 | Modelo LLM |
| `embedding_model` | `load_vectorstore.py` | 39 | all-MiniLM-L12-v2 | Modelo de embeddings |

## Comandos √öteis

### Setup Inicial

```bash
# 1. Criar ambiente virtual
python -m venv venv

# 2. Ativar ambiente virtual
# Linux/Mac:
source venv/bin/activate
# Windows:
venv\Scripts\activate

# 3. Instalar depend√™ncias
pip install -r server/requirements.txt

# 4. Criar arquivo .env
echo "GROQ_API_KEY=sua_chave_aqui" > .env
```

### Desenvolvimento

**Rodar Backend (FastAPI):**
```bash
# Entrar no diret√≥rio do servidor
cd server

# Rodar com reload autom√°tico (desenvolvimento)
uvicorn main:app --reload --host 127.0.0.1 --port 8000

# Rodar sem reload (produ√ß√£o)
uvicorn main:app --host 0.0.0.0 --port 8000
```

**Rodar Frontend (Streamlit):**
```bash
# Em terminal separado, entrar no diret√≥rio do cliente
cd client

# Rodar Streamlit
streamlit run app.py

# Rodar em porta espec√≠fica
streamlit run app.py --server.port 8501
```

**Rodar ambos simultaneamente:**
```bash
# Terminal 1: Backend
cd server && uvicorn main:app --reload

# Terminal 2: Frontend
cd client && streamlit run app.py
```

### Testes e Debug

**Testar API:**
```bash
# Health check
curl http://127.0.0.1:8000/test

# Ver documenta√ß√£o interativa
# Abrir no navegador: http://127.0.0.1:8000/docs
```

**Debug com logs detalhados:**
```python
# Em logger.py, ajustar n√≠vel:
logger.setLevel(logging.DEBUG)  # J√° configurado
```

**Limpar dados:**
```bash
# Remover vectorstore (for√ßa reindexa√ß√£o)
rm -rf chroma_store/

# Remover PDFs salvos
rm -rf uploaded_pdfs/
```

### Gerenciamento de Depend√™ncias

**Atualizar requirements.txt:**
```bash
pip freeze > server/requirements.txt
```

**Instalar nova depend√™ncia:**
```bash
pip install nome-do-pacote
pip freeze > server/requirements.txt
```

## Conven√ß√µes de C√≥digo

### Estilo Python

**Seguir PEP 8:**
- Indenta√ß√£o: 4 espa√ßos
- Linhas: m√°ximo 100 caracteres (preferencialmente 88)
- Imports: agrupados (stdlib, third-party, local)
- Naming:
  - Fun√ß√µes/vari√°veis: `snake_case`
  - Classes: `PascalCase`
  - Constantes: `UPPER_SNAKE_CASE`

**Exemplo:**
```python
from typing import List
from langchain_core.documents import Document

PERSIST_DIR = "./chroma_store"  # Constante

def process_documents(documents: List[Document]) -> None:
    """Processa lista de documentos."""
    pass
```

### Organiza√ß√£o de M√≥dulos

**Backend (server/):**
- Um m√≥dulo = uma responsabilidade
- Fun√ß√µes devem ser puras quando poss√≠vel
- Evitar l√≥gica de neg√≥cio em `main.py`
- M√≥dulos em `modules/` para funcionalidades RAG

**Frontend (client/):**
- Componentes em `components/`
- Cada componente = uma fun√ß√£o `render_*()`
- Utilit√°rios em `utils/`
- Configura√ß√µes em `config.py`

### Nomenclatura de Fun√ß√µes

**Padr√µes existentes:**
- `process_*` - Transforma√ß√£o de dados (ex: `process_uploaded_pdf`)
- `add_*_to_*` - Adiciona item a cole√ß√£o (ex: `add_documents_to_vectorstore`)
- `get_*` - Retorna objeto configurado (ex: `get_llm_chain`)
- `query_*` - Executa consulta (ex: `query_chain`)
- `render_*` - Renderiza UI (ex: `render_chat`)
- `setup_*` - Inicializa√ß√£o (ex: `setup_logger`)

### Type Hints

**Sempre usar type hints:**
```python
from typing import List, Optional
from langchain_core.documents import Document

def process_pdf(file: UploadFile) -> List[Document] | None:
    """Type hints tornam c√≥digo mais claro e detectam bugs."""
    pass
```

### Docstrings

**Formato:**
```python
def function_name(param: Type) -> ReturnType:
    """
    Breve descri√ß√£o em uma linha.

    Descri√ß√£o mais detalhada se necess√°rio.

    Args:
        param: Descri√ß√£o do par√¢metro.

    Returns:
        Descri√ß√£o do que √© retornado.
    """
    pass
```

### Logging

**N√≠veis de log:**
```python
log.debug("Informa√ß√£o detalhada para debug")
log.info("Confirma√ß√£o de opera√ß√£o normal")
log.warning("Algo inesperado mas n√£o cr√≠tico")
log.error("Erro que impediu opera√ß√£o")
log.exception("Erro com stacktrace completo")
```

**Quando logar:**
- `INFO`: In√≠cio/fim de opera√ß√µes importantes
- `WARNING`: Fallbacks, dados faltantes n√£o cr√≠ticos
- `ERROR`: Falhas que impedem opera√ß√£o
- `DEBUG`: Detalhes internos (valores de vari√°veis)

### Tratamento de Erros

**Padr√µes:**
```python
# Em endpoints FastAPI: usar HTTPException
from fastapi import HTTPException

if chain is None:
    raise HTTPException(
        status_code=400,
        detail="Chain n√£o inicializada. Fa√ßa upload de PDFs."
    )

# Em fun√ß√µes: logar e repassar ou retornar None
try:
    result = operation()
except Exception as e:
    log.exception(f"Erro em operation: {e}")
    raise  # ou return None
```

### Estrutura de Endpoints

**Padr√£o FastAPI:**
```python
@app.post("/endpoint/")
async def endpoint_name(param: Type = Form(...)):
    """
    Docstring descrevendo o endpoint.
    """
    global chain  # Se necess√°rio

    # 1. Valida√ß√£o
    if not valid:
        raise HTTPException(status_code=400, detail="...")

    # 2. Logging
    log.info(f"Opera√ß√£o iniciada: {param}")

    # 3. Processamento
    try:
        result = process(param)
    except Exception as e:
        log.exception("Erro no processamento")
        raise HTTPException(status_code=500, detail=str(e))

    # 4. Retorno
    log.info("Opera√ß√£o conclu√≠da com sucesso")
    return {"message": "Sucesso", "data": result}
```

### Componentes Streamlit

**Padr√£o:**
```python
def render_component_name():
    """
    Renderiza [descri√ß√£o do componente].
    """
    # 1. Inicializar session_state se necess√°rio
    if "key" not in st.session_state:
        st.session_state.key = default_value

    # 2. UI
    st.header("T√≠tulo")
    user_input = st.input_widget()

    # 3. L√≥gica de intera√ß√£o
    if st.button("A√ß√£o"):
        result = api_call(user_input)
        st.success("Sucesso!")
```

## Notas Importantes

### üî¥ Pontos Cr√≠ticos

1. **Chain Global State**
   - A vari√°vel `chain` em `server/main.py` √© global e stateful
   - DEVE ser recriada ap√≥s cada upload de PDFs (linha 87)
   - Sem isso, novas queries n√£o veem documentos adicionados

2. **Consist√™ncia de Embeddings**
   - SEMPRE usar o MESMO modelo de embeddings
   - Atualmente: `all-MiniLM-L12-v2`
   - Mudar modelo requer reindexa√ß√£o completa (deletar `chroma_store/`)

3. **GROQ_API_KEY Obrigat√≥ria**
   - Sistema n√£o funciona sem a chave
   - Carregar com `python-dotenv` antes de inicializar LLM
   - Nunca comitar `.env` no git

4. **Ordem de Inicializa√ß√£o**
   - Backend DEVE iniciar antes do frontend
   - Frontend assume que `http://127.0.0.1:8000` est√° dispon√≠vel

5. **ChromaDB Persistence**
   - ChromaDB persiste automaticamente em disco
   - Diret√≥rio `chroma_store/` cont√©m √≠ndices e dados
   - Deletar = perder todo o conhecimento indexado

### ‚ö†Ô∏è Limita√ß√µes Conhecidas

1. **Modelo de Embeddings em CPU**
   - `all-MiniLM-L12-v2` roda em CPU por padr√£o
   - Para GPU: modificar `model_kwargs={'device': 'cuda'}`
   - Upload de muitos PDFs pode ser lento

2. **Sem Autentica√ß√£o**
   - API n√£o tem autentica√ß√£o
   - Qualquer cliente pode acessar
   - N√£o usar em produ√ß√£o sem adicionar auth

3. **CORS Aberto**
   - `allow_origins=["*"]` permite qualquer origem
   - OK para desenvolvimento
   - Em produ√ß√£o: especificar origens permitidas

4. **Sem Rate Limiting**
   - Groq API tem limites de requisi√ß√µes
   - Sistema n√£o implementa retry ou rate limiting
   - Pode falhar em uso intenso

5. **Session State Vol√°til**
   - Hist√≥rico do chat vive apenas em `st.session_state`
   - Refresh da p√°gina = perde hist√≥rico
   - Para persist√™ncia: salvar em banco de dados

### üõ†Ô∏è Debugging Comum

**Problema: "Chain n√£o inicializada"**
- Causa: Vectorstore n√£o existe
- Solu√ß√£o: Fazer upload de pelo menos um PDF

**Problema: "Embeddings incompat√≠veis"**
- Causa: Mudou modelo de embeddings mas vectorstore antigo existe
- Solu√ß√£o: `rm -rf chroma_store/` e reindexar

**Problema: Frontend n√£o conecta ao backend**
- Causa: Backend n√£o est√° rodando ou porta errada
- Solu√ß√£o: Verificar `client/config.py` e rodar backend

**Problema: Respostas irrelevantes**
- Causa: k muito baixo ou chunks muito pequenos
- Solu√ß√£o: Aumentar k em `llm.py:24` ou chunk_size em `load_vectorstore.py:33`

**Problema: Erro "GROQ_API_KEY not found"**
- Causa: Arquivo `.env` n√£o existe ou est√° mal formatado
- Solu√ß√£o: Criar `.env` com formato correto

### üìà Melhorias Futuras Sugeridas

1. **Autentica√ß√£o e Autoriza√ß√£o**
   - Adicionar FastAPI OAuth2 ou JWT
   - Separar documentos por usu√°rio

2. **Persist√™ncia de Hist√≥rico**
   - Salvar conversas em banco de dados
   - Permitir retomar conversas antigas

3. **Rate Limiting**
   - Implementar slowapi ou similar
   - Prevenir abuso da API Groq

4. **Testes Automatizados**
   - Unit tests para fun√ß√µes de processamento
   - Integration tests para endpoints
   - Framework: pytest

5. **Melhor Tratamento de Chunks**
   - Considerar semantic chunking
   - Implementar re-ranking de resultados

6. **Monitoring**
   - Adicionar m√©tricas (lat√™ncia, uso)
   - Dashboard de observabilidade

7. **Deployment**
   - Dockerizar aplica√ß√£o
   - CI/CD pipeline
   - Environment configs (dev/staging/prod)

### üîß Modifica√ß√µes Comuns

**Mudar modelo LLM:**
```python
# Em server/modules/llm.py:17
llm = ChatGroq(
    groq_api_key=os.getenv('GROQ_API_KEY'),
    model_name='llama3-8b-8192',  # Modelo menor/mais r√°pido
    temperature=0.2
)
```

**Ajustar n√∫mero de chunks recuperados:**
```python
# Em server/modules/llm.py:24
retriever = vectorstore.as_retriever(search_kwargs={'k': 5})  # Mais contexto
```

**Usar GPU para embeddings:**
```python
# Em server/modules/load_vectorstore.py:39
embeddings = HuggingFaceEmbeddings(
    model_name="all-MiniLM-L12-v2",
    model_kwargs={'device': 'cuda'}  # Requer CUDA instalado
)
```

**Adicionar novo endpoint:**
```python
# Em server/main.py
@app.get("/list_documents/")
async def list_documents():
    """Lista todos os PDFs indexados."""
    if not os.path.exists(UPLOAD_DIR):
        return {"documents": []}
    files = os.listdir(UPLOAD_DIR)
    return {"documents": [f for f in files if f.endswith('.pdf')]}
```

### üìö Recursos Adicionais

**Documenta√ß√£o Oficial:**
- LangChain: https://python.langchain.com/docs/
- FastAPI: https://fastapi.tiangolo.com/
- Streamlit: https://docs.streamlit.io/
- ChromaDB: https://docs.trychroma.com/
- Groq: https://console.groq.com/docs/

**Conceitos RAG:**
- RAG Overview: https://www.pinecone.io/learn/retrieval-augmented-generation/
- Vector Embeddings: https://www.pinecone.io/learn/vector-embeddings/
- Semantic Search: https://www.sbert.net/examples/applications/semantic-search/README.html

---

**√öltima atualiza√ß√£o**: 2025-10-12
**Vers√£o**: 2.0
